# import libraries
try:
  # %tensorflow_version only exists in Colab.
  !pip install tf-nightly
except Exception:
  pass
import tensorflow as tf
import pandas as pd
from tensorflow import keras
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
!pip install tensorflow-datasets
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
%matplotlib inline
from sklearn.preprocessing import LabelEncoder
print(tf.__version__)

# get data files
!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv
!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv

train_file_path = "train-data.tsv"
test_file_path = "valid-data.tsv"

train_dataset= pd.read_csv(train_file_path, sep="\t", header=None, names=["spam_ham","message"])
test_dataset= pd.read_csv(test_file_path, sep="\t", header=None, names=["spam_ham","message"])
train_dataset["target"]=LabelEncoder().fit_transform(train_dataset["spam_ham"].astype("str"))
test_dataset["target"]=LabelEncoder().fit_transform(test_dataset["spam_ham"].astype("str"))
train_dataset, test_dataset

train_dataset["len"]=[len(x) for x in train_dataset["message"]]
train_spam_dataset=train_dataset.loc[train_dataset["target"]==1]
train_ham_dataset= train_dataset.loc[train_dataset["target"]==0]
train_ham_dataset, train_spam_dataset
#We can see that usually, the lenght of SMS in the spam is larger then in non-spam messages

#Create a numpy list to visualizate the most commun words in the non-spam text
ham_text="".join((train_ham_dataset["message"]).to_numpy().tolist())
ham_text_cloud = WordCloud(width=520, height=260, stopwords=STOPWORDS, max_font_size=50, background_color="blue", colormap="gray").generate(ham_text)
plt.figure(figsize=(16,10))
plt.imshow(ham_text_cloud, interpolation="bilinear")
plt.show()

#Create a numpy list to visualizate the most commun words in the spam text
spam_text= "".join((train_spam_dataset["message"]).to_numpy().tolist())
spam_text_cloud= WordCloud (width=520, height=260, stopwords=STOPWORDS, max_font_size=50, background_color="blue", colormap="gray" ).generate(spam_text)
plt.figure (figsize=(16,10))
plt.imshow(spam_text_cloud, interpolation="bilinear")
plt.show()

x_train=train_dataset["message"]
x_test=test_dataset["message"]
y_train=train_dataset["target"]
y_test=test_dataset["target"]

#Now, as we need to process the data and we have strings we can Tokenizer the words, to later, pad sequences.
#We nedd to use some parameters to Tokenizer: number of words in the vocabulary 500; char_level=False because we want words instead of chars.
# maxlen=50 because we pad 50 words in 50 words, and padding a posteriori instead of at the beginning.

tokens=Tokenizer(num_words=5000, char_level=False, oov_token="<OOV>")
tokens.fit_on_texts(x_train)

training_sequences= tokens.texts_to_sequences(x_train)
training_padded= pad_sequences(training_sequences, maxlen=50, padding="post", truncating="post")
test_sequences= tokens.texts_to_sequences(x_test)
test_padded= pad_sequences (test_sequences, maxlen=50, padding="post", truncating="post")

#We can print the padded first message:
print(training_sequences[0])
print(training_padded[0])

#We create the architecture of the Dense  spam Detection model
vocab_size=5000
embeding_dim=50
drop_value=0.5
n_dense=24

model= keras.models.Sequential()
model.add(Embedding(vocab_size, embeding_dim, input_length=50))
model.add(keras.layers.GlobalAveragePooling1D())
model.add(Dense(256, activation="relu"))
model.add(Dropout(drop_value))
model.add(Dense(1, activation="sigmoid"))
model.summary()

model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
history= model.fit(training_padded, y_train, epochs=20, validation_data=(test_padded, y_test), verbose=1)

metrics=pd.DataFrame(history.history)
def plot_graph (var1, var2, string):
  metrics[[var1, var2]].plot()
  plt.title("Dense Classifier: Training & Validation "+ string)
  plt.xlabel("Number of epochs")
  plt.ylabel(string)
  plt.legend([var1, var2])
plot_graph("loss", "val_loss", "loss")
plot_graph("accuracy", "val_accuracy", "accuracy")

def processing(x):
  i=x.apply(lambda i: i.lower()) 
  i=tokens.texts_to_sequences(i)
  return pad_sequences(i, maxlen=50, padding="post", truncating="post")
  
# function to predict messages based on model
# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])

def predict_message(pred_text):
  pred=model.predict(processing(pd.Series([pred_text])))[0]
  return (pred[0],( "ham" if pred<=0.5 else "spam"))
pred_text= "if you do not wish more promotions, to stop service text 322"
prediction=predict_message(pred_text) 
print(prediction)  

# Run this cell to test your function and model. Do not modify contents.
def test_predictions():
  test_messages = ["how are you doing today",
                   "sale today! to stop texts call 98912460324",
                   "i dont want to go. can we try it a different day? available sat",
                   "our new mobile video service is live. just install on your phone to start watching.",
                   "you have won Â£1000 cash! call to claim your prize.",
                   "i'll bring it tomorrow. don't forget the milk.",
                   "wow, is your arm alright. that happened to me one time too"
                  ]

  test_answers = ["ham", "spam", "ham", "spam", "spam", "ham", "ham"]
  passed = True
    for msg, ans in zip(test_messages, test_answers):
    prediction = predict_message(msg)
    if prediction[1] != ans:
      passed = False

  if passed:
    print("You passed the challenge. Great job!")
  else:
    print(prediction[0], prediction[1])
    print("You haven't passed yet. Keep trying.")

test_predictions()
